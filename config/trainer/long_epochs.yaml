skip_training: False
auto_resume_from_checkpoint: False
enable_final_evaluation: True

idx_fold: 0

train:
  batch_size: 256

evaluation:
  batch_size: 256
  save_predictions: True
  dirpath: "${save_dir}/predictions/oof/${experiment_name}"
  filename: "fold_${trainer.idx_fold}.npy"

inference:
  save_predictions: True
  dirpath: "${save_dir}/predictions/test/${experiment_name}"
  filename: "fold_${trainer.idx_fold}.npy"

trainer:
  resume_from_checkpoint: # '${save_dir}/models/${experiment_name}/fold_${trainer.idx_fold}_best.ckpt'
  max_epochs: 15
  check_val_every_n_epoch: 1
  accumulate_grad_batches: 1
  gradient_clip_val: 10
  amp_backend: "native"
  amp_level: "O1"
  precision: 16
  gpus: -1
  accelerator: "ddp"
  # sync_batchnorm: True
  benchmark: True
  deterministic: True
  num_sanity_val_steps: 0
  # track_grad_norm: 2
  # stochastic_weight_avg: True

logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  project: ${project}
  name: ${experiment_name}

callbacks:
  ModelCheckpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val_roc_overall"
    mode: "max"
    save_last: False
    save_top_k: 1
    dirpath: "${save_dir}/models/${experiment_name}"
    filename: "fold_${trainer.idx_fold}_{epoch:03d}"
